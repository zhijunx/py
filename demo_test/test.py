#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ–°é—»è·å–è„šæœ¬
æ”¯æŒå¤šä¸ªæ–°é—»æºï¼Œå¯å®šæ—¶è¿è¡Œ
"""

import requests
from datetime import datetime
import json
import time
from bs4 import BeautifulSoup
import schedule

class NewsAggregator:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def get_zhihu_hot(self, limit=10):
        """è·å–çŸ¥ä¹çƒ­æ¦œ"""
        try:
            url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total"
            params = {'limit': limit}
            response = requests.get(url, headers=self.headers, params=params, timeout=10)
            data = response.json()

            news_list = []
            for item in data.get('data', [])[:limit]:
                news_list.append({
                    'title': item['target']['title'],
                    'excerpt': item['target'].get('excerpt', ''),
                    'url': f"https://www.zhihu.com/question/{item['target']['id']}"
                })
            return news_list
        except Exception as e:
            print(f"è·å–çŸ¥ä¹çƒ­æ¦œå¤±è´¥: {e}")
            return []

    def get_weibo_hot(self, limit=10):
        """è·å–å¾®åšçƒ­æœ"""
        try:
            url = "https://weibo.com/ajax/side/hotSearch"
            response = requests.get(url, headers=self.headers, timeout=10)
            data = response.json()

            news_list = []
            for item in data.get('data', {}).get('realtime', [])[:limit]:
                news_list.append({
                    'title': item.get('note', ''),
                    'hot_value': item.get('num', 0),
                    'url': f"https://s.weibo.com/weibo?q=%23{item.get('word', '')}%23"
                })
            return news_list
        except Exception as e:
            print(f"è·å–å¾®åšçƒ­æœå¤±è´¥: {e}")
            return []

    def get_36kr_news(self, limit=10):
        """è·å–36æ°ªå¿«è®¯"""
        try:
            url = "https://36kr.com/api/newsflash"
            response = requests.get(url, headers=self.headers, timeout=10)
            data = response.json()

            news_list = []
            for item in data.get('data', {}).get('items', [])[:limit]:
                news_list.append({
                    'title': item.get('title', ''),
                    'summary': item.get('summary', ''),
                    'time': item.get('published_at', ''),
                    'url': f"https://36kr.com/newsflashes/{item.get('id', '')}"
                })
            return news_list
        except Exception as e:
            print(f"è·å–36æ°ªæ–°é—»å¤±è´¥: {e}")
            return []

    def format_news_report(self, zhihu_news, weibo_news, kr_news):
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append(f"ğŸ“° æ¯æ—¥æ–°é—»æ—©æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %A')}")
        report.append("=" * 60)
        report.append("")

        if zhihu_news:
            report.append("ğŸ”¥ çŸ¥ä¹çƒ­æ¦œ TOP 10")
            report.append("-" * 60)
            for i, news in enumerate(zhihu_news, 1):
                report.append(f"{i}. {news['title']}")
                if news['excerpt']:
                    report.append(f"   æ‘˜è¦: {news['excerpt'][:100]}...")
                report.append(f"   é“¾æ¥: {news['url']}")
                report.append("")

        if weibo_news:
            report.append("ğŸ”¥ å¾®åšçƒ­æœ TOP 10")
            report.append("-" * 60)
            for i, news in enumerate(weibo_news, 1):
                report.append(f"{i}. {news['title']} (çƒ­åº¦: {news['hot_value']})")
                report.append(f"   é“¾æ¥: {news['url']}")
                report.append("")

        if kr_news:
            report.append("ğŸ’¼ 36æ°ªå¿«è®¯")
            report.append("-" * 60)
            for i, news in enumerate(kr_news, 1):
                report.append(f"{i}. {news['title']}")
                if news['summary']:
                    report.append(f"   {news['summary'][:100]}...")
                report.append(f"   æ—¶é—´: {news['time']}")
                report.append("")

        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)

        return "\n".join(report)

    def save_to_file(self, content, filename=None):
        """ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"news_{datetime.now().strftime('%Y%m%d')}.txt"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… æ–°é—»å·²ä¿å­˜åˆ°: {filename}")

    def fetch_daily_news(self):
        """è·å–æ¯æ—¥æ–°é—»"""
        print(f"\nâ° å¼€å§‹è·å–æ–°é—»... {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # è·å–å„å¹³å°æ–°é—»
        zhihu_news = self.get_zhihu_hot(10)
        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

        weibo_news = self.get_weibo_hot(10)
        time.sleep(1)

        kr_news = self.get_36kr_news(10)

        # æ ¼å¼åŒ–å¹¶ä¿å­˜
        report = self.format_news_report(zhihu_news, weibo_news, kr_news)
        print(report)

        # ä¿å­˜åˆ°æ–‡ä»¶
        self.save_to_file(report)

        return report

def run_scheduler():
    """è¿è¡Œå®šæ—¶ä»»åŠ¡"""
    aggregator = NewsAggregator()

    # è®¾ç½®æ¯å¤©æ—©ä¸Š8ç‚¹è¿è¡Œ
    schedule.every().day.at("08:00").do(aggregator.fetch_daily_news)

    print("ğŸ“… å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯å¤©æ—©ä¸Š8:00è‡ªåŠ¨è·å–æ–°é—»")
    print("ğŸ’¡ ä¹Ÿå¯ä»¥æŒ‰ Ctrl+C åœæ­¢ç¨‹åº")

    # ç«‹å³è¿è¡Œä¸€æ¬¡
    aggregator.fetch_daily_news()

    # ä¿æŒè¿è¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    # æ–¹å¼1: ç«‹å³è¿è¡Œä¸€æ¬¡
    print("MAIN ENTRY !!!")
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. ç«‹å³è·å–ä¸€æ¬¡æ–°é—»")
    print("2. å¯åŠ¨å®šæ—¶ä»»åŠ¡(æ¯å¤©æ—©ä¸Š8ç‚¹)")

    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()

    aggregator = NewsAggregator()

    if choice == "1":
        aggregator.fetch_daily_news()
    elif choice == "2":
        run_scheduler()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ç«‹å³è¿è¡Œä¸€æ¬¡")
        aggregator.fetch_daily_news()